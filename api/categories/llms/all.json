[{"category":"LLMs","homepage_url":"https://homepage.url","id":"llms--serving--vllm","logo_url":"http://127.0.0.1:8000/logos/789d223c86d7884c48ea51f8e799e8c0ce952495186131fcaed24c06292d0e45.svg","name":"vLLM","subcategory":"Serving","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","languages":{"C":95214,"C++":1251440,"CMake":98243,"Cuda":2010123,"Dockerfile":35685,"HCL":1731,"Jinja":6380,"Python":26045994,"Shell":255325},"primary":true}]}]