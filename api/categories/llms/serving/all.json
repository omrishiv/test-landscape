[{"category":"LLMs","homepage_url":"https://www.sglang.io/","id":"llms--serving--sglang","logo_url":"http://127.0.0.1:8000/logos/c801ac352c84d27689070962d2d8bf4d042c369759bac9dfa40d89eac48c027e.png","name":"SGLang","subcategory":"Serving","description":"SGLang is a high-performance serving framework for large language models and multimodal models.","oss":true,"repositories":[{"url":"https://github.com/sgl-project/sglang","languages":{"C":100244,"C++":1365286,"CMake":32407,"Cuda":1580046,"Dockerfile":60409,"Go":105506,"HIP":15490,"Jinja":1205,"Jupyter Notebook":7292,"Makefile":23168,"Python":24984214,"Rust":3095858,"Shell":151167,"Vim Script":914},"primary":true}]},{"category":"LLMs","homepage_url":"https://vllm.ai/","id":"llms--serving--vllm","logo_url":"http://127.0.0.1:8000/logos/789d223c86d7884c48ea51f8e799e8c0ce952495186131fcaed24c06292d0e45.svg","name":"vLLM","subcategory":"Serving","description":"A high-throughput and memory-efficient inference and serving engine for LLMs","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","languages":{"C":95214,"C++":1251440,"CMake":98243,"Cuda":2010123,"Dockerfile":35685,"HCL":1731,"Jinja":6380,"Python":26045994,"Shell":255325},"primary":true}]}]