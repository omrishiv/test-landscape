{"github_data":{"https://github.com/vllm-project/vllm":{"contributors":{"count":2178,"url":"https://github.com/vllm-project/vllm/graphs/contributors"},"description":"A high-throughput and memory-efficient inference and serving engine for LLMs","generated_at":"2026-02-10T17:39:03.857110903Z","latest_commit":{"ts":"2026-02-10T16:55:22Z","url":"https://github.com/vllm-project/vllm/commit/a2443de5fa4a0605607f6c3d9219022c7f6ac480"},"participation_stats":[132,130,126,163,165,143,159,153,146,107,192,154,149,157,165,137,141,125,113,109,146,173,155,224,197,199,198,203,205,154,248,267,247,197,201,214,167,170,188,244,246,205,227,222,190,107,94,264,167,205,229,224],"stars":69986,"topics":["amd","blackwell","cuda","deepseek","deepseek-v3","gpt","gpt-oss","inference","kimi","llama","llm","llm-serving","model-serving","moe","openai","pytorch","qwen","qwen3","tpu","transformer"],"url":"https://github.com/vllm-project/vllm","first_commit":{"ts":"2023-02-09T11:24:15Z","url":"https://github.com/vllm-project/vllm/commit/e7d9d9c08c79b386f6d0477e87b77a572390317d"},"languages":{"C":95214,"C++":1251440,"CMake":98243,"Cuda":2010123,"Dockerfile":35685,"HCL":1731,"Jinja":6380,"Python":26045994,"Shell":255325},"latest_release":{"ts":"2026-02-04T20:48:08Z","url":"https://github.com/vllm-project/vllm/releases/tag/v0.15.1"},"license":"Apache License 2.0"}},"items":[{"category":"LLMs","homepage_url":"https://homepage.url","id":"llms--serving--vllm","logo":"logos/789d223c86d7884c48ea51f8e799e8c0ce952495186131fcaed24c06292d0e45.svg","name":"vLLM","subcategory":"Serving","website":"https://homepage.url","oss":true,"repositories":[{"url":"https://github.com/vllm-project/vllm","primary":true}]}]}